victorops {
  example {
    cluster-name = "ExampleCluster"
    cluster-shards = 100
  }

  cassandra {
    username = cassandra
    password = password
  }
}

akka {
  actor {
    provider = cluster
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://"${victorops.example.cluster-name}"@127.0.0.1:2551",
      "akka.tcp://"${victorops.example.cluster-name}"@127.0.0.1:2552"
    ]

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    #
    # auto-down-unreachable-after = 10s
  }
}

# Disable legacy metrics in akka-cluster.
akka.cluster.metrics.enabled=off

# Enable metrics extension in akka-cluster-metrics.
//akka.extensions=["akka.cluster.metrics.ClusterMetricsExtension"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
//akka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native

# Settings for the ClusterShardingExtension

akka {
  cluster {
    sharding {

      # The extension creates a top level actor with this name in top level system scope,
      # e.g. '/system/sharding'
      guardian-name = sharding

      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = ""

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration, with an exponential back-off
      # of up to 5 times this duration.
      coordinator-failure-backoff = 5 s

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s

      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000

      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # Time given to a region to acknowledge it's hosting a shard.
      shard-start-timeout = 10 s

      # If the shard is remembering entities and can't store state changes
      # will be stopped and then started again after this duration. Any messages
      # sent to an affected entity may be lost in this process.
      shard-failure-backoff = 10 s

      # If the shard is remembering entities and an entity stops itself without
      # using passivate. The entity will be restarted after this duration or when
      # the next message for it is received, which ever occurs first.
      entity-restart-backoff = 10 s

      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      journal-plugin-id = "cassandra-journal"

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      snapshot-plugin-id = "cassandra-snapshot-store"

      # Parameter which determines how the coordinator will be store a state
      # valid values either "persistence" or "ddata"
      # The "ddata" mode is experimental, since it depends on the experimental
      # module akka-distributed-data-experimental.
      state-store-mode = "persistence"

      # The shard saves persistent snapshots after this number of persistent
      # events. Snapshots are used to reduce recovery times.
      snapshot-after = 1000

      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10

        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }

      # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
      # works only for state-store-mode = "ddata"
      waiting-for-state-timeout = 5 s

      # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
      # works only for state-store-mode = "ddata"
      updating-state-timeout = 5 s

      # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
      # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
      # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
      # entity actors at a fix rate. The default strategy "all".
      entity-recovery-strategy = "all"

      # Default settings for the constant rate entity recovery strategy
      entity-recovery-constant-rate-strategy {
        # Sets the frequency at which a batch of entity actors is started.
        frequency = 100 ms
        # Sets the number of entity actors to be restart at a particular interval
        number-of-entities = 5
      }

      # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
      # The "role" of the singleton configuration is not used. The singleton role will
      # be the same as "akka.cluster.sharding.role".
      coordinator-singleton = ${akka.cluster.singleton}

      # The id of the dispatcher to use for ClusterSharding actors.
      # If not specified default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      # This dispatcher for the entity actors is defined by the user provided
      # Props, i.e. this dispatcher is not used for the entity actors.
      use-dispatcher = ""
    }
  }
}

cassandra-journal {
  contact-points = [127.0.0.1]
  keyspace = "cluster_example_akka_journal"
  authentication.username = ${victorops.cassandra.username}
  authentication.password = ${victorops.cassandra.password}
  cassandra-2x-compat = on

  # DANGER: DO NOT CHANGE THE target-partition-size SETTING!
  target-partition-size = 50000
}

cassandra-snapshot-store {
  contact-points = [127.0.0.1]
  keyspace = "cluster_example_akka_snapshot"
  authentication.username = ${victorops.cassandra.username}
  authentication.password = ${victorops.cassandra.password}
  cassandra-2x-compat = on
}
